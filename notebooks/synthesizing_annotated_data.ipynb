{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f975034658bf429b8184e6461c0000a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'label_text'],\n",
      "    num_rows: 2264\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", split='train', trust_remote_code=True).shuffle(seed=42)\n",
    "\n",
    "#dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)\n",
    "\n",
    "# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\n",
    "label_map = {\n",
    "    i: label_text \n",
    "    for i, label_text in enumerate(dataset.features[\"label\"].names)\n",
    "}\n",
    "\n",
    "def add_label_text(example):\n",
    "    example[\"label_text\"] = label_map[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_label_text)\n",
    "\n",
    "print(dataset)\n",
    "# Dataset({\n",
    "#    features: ['sentence', 'label', 'label_text'],\n",
    "#    num_rows: 2264\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label_text'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"You are a highly qualified expert trained to annotate machine learning training data.\"\n",
    "prompt_financial_sentiment = \"\"\"\\\n",
    "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
    "positive, negative, or neutral.\n",
    "\n",
    "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
    "\n",
    "Do not provide any explanations and ONLY respond with one of the labels as one word: negative, positive, or neutral\n",
    "\n",
    "Examples:\n",
    "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
    "Label: positive\n",
    "Text: The company generated net sales of 11.3 million euro this year.\n",
    "Label: neutral\n",
    "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
    "Label: negative\n",
    "\n",
    "Your TEXT to analyse:\n",
    "TEXT: {text}\n",
    "Label: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_financial_sentiment = \"\"\"\\\n",
    "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
    "positive, negative, or neutral.\n",
    "\n",
    "ONLY respond with one of the labels as one word: negative, positive, or neutral. DONT BRING ANYTHING ELSE IN THE ANSWER, JUST THE LABEL\n",
    "\n",
    "Examples:\n",
    "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
    "Answer: positive\n",
    "Text: The company generated net sales of 11.3 million euro this year.\n",
    "Answer: neutral\n",
    "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
    "Answer: negative\n",
    "\n",
    "Your TEXT to analyse:\n",
    "TEXT: {text}\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "# chat_financial_sentiment = [{\"role\": \"user\", \"content\": prompt_financial_sentiment}]\n",
    "\n",
    "# prompt_financial_sentiment = tokenizer.apply_chat_template(chat_financial_sentiment, tokenize=False)\n",
    "\n",
    "# The prompt now includes special tokens: '<s>[INST] You are a highly qualified expert ...  [/INST]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d173fe0f98c74b4e8449fe1ac3315c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" #\"microsoft/Phi-3-mini-4k-instruct\"\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    model_id,  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "def clean_output(outputs):\n",
    "    labels = []\n",
    "    for out in outputs:\n",
    "        text = out[0]['generated_text']  # adjust if your structure is different\n",
    "        # find all occurrences of \"Label: <label>\"\n",
    "        found = re.findall(r'Label:\\s*(positive|negative|neutral)', text, re.IGNORECASE)\n",
    "        if found:\n",
    "            # take the first label (or apply other logic)\n",
    "            labels.append(found[0].lower())\n",
    "        else:\n",
    "            labels.append(None)  # or a default\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Bananas and dragonfruits can be combined in various delicious ways. Here are some creative ideas for incorporating both fruits into your meals or snacks:\n",
      "\n",
      "1. Smoothie: Blend together a ripe banana, a few slices of dragon fruit, a handful of spinach or kale, a splash of almond milk, and a tablespoon of honey or agave syrup for sweetness. Add a scoop of your favorite protein powder or a handful of ice for a refreshing and nutritious smoothie.\n",
      "\n",
      "2. Fruit Salad: Slice a ripe banana and a few pieces of dragon fruit, and combine them with other fruits like strawberries, blueberries, and kiwi. Toss the fruits with a drizzle of honey and a squeeze of lime juice for a colorful and flavorful fruit salad.\n",
      "\n",
      "3. Tropical Salsa: Dice a ripe banana and a few pieces of dragon fruit, and combine them with diced mango, pineapple, and red bell pepper. Add a squeeze of lime juice, a drizzle of honey, and a sprinkle of chopped cilantro for a sweet and tangy tropical salsa. Serve with tortilla chips or as a topping for grilled chicken or fish.\n",
      "\n",
      "4. Breakfast Bowl: Top a bowl of oatmeal or yogurt with sliced bananas and dragon fruit. Add a drizzle of honey, a sprinkle of cinnamon, and a handful of chopped nuts or seeds for a nutritious and satisfying breakfast bowl.\n",
      "\n",
      "5. Dessert Parfait: Layer slices of banana and dragon fruit with Greek yogurt, granola, and a drizzle of honey in a glass or jar. Repeat the layers until the glass is filled, and top with a sprinkle of chopped nuts or coconut flakes for a delicious and healthy dessert parfait.\n",
      "\n",
      "6. Fruit Skewers: Thread slices of banana and dragon fruit onto\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "] \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.9, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Fortum expects its annual capital expenditure in the next four to five years to be within a range of EUR 0.8-1 .2 billion , as earlier announced .\n",
      "Prediction: neutral\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "one_shot_text = dataset[\"sentence\"][4]\n",
    "\n",
    "generation_params = dict(\n",
    "    top_p=0.95,\n",
    "    temperature=0.4,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "prompt_formatted = prompt_financial_sentiment.format(text=one_shot_text)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a highly qualified expert trained to annotate machine learning training data.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_formatted},\n",
    "]\n",
    "\n",
    "output = pipe(messages, **generation_params)\n",
    "\n",
    "\n",
    "#output = pipe(prompt_formatted, **generation_params)\n",
    "label = clean_output([output])\n",
    "print(f\"Input: {one_shot_text}\")\n",
    "print(f\"Prediction: {output[0]['generated_text'].strip()}\")\n",
    "print(\"-\" * 40)\n",
    "# print(f\"Text: {one_shot_text}\\nLabel: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---- Build batch prompts (messages style) ----\n",
    "batch_messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a highly qualified expert trained to annotate machine learning training data.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_financial_sentiment.format(text=text)},\n",
    "    ]\n",
    "    for text in dataset[\"sentence\"][:10]\n",
    "]\n",
    "\n",
    "# ---- Run batch inference ----\n",
    "raw_outputs = pipe(batch_messages, **generation_params)\n",
    "\n",
    "# ---- Extract labels ----\n",
    "predicted_labels = clean_output(raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'negative',\n",
       " 'neutral',\n",
       " 'negative']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Cleaning function (maps to labels) ----\n",
    "def clean_output(outputs, labels=(\"positive\", \"negative\", \"neutral\")):\n",
    "    results = []\n",
    "    for out in outputs:\n",
    "        text = out[0][\"generated_text\"].strip()  # each out is a list of dicts\n",
    "        found = None\n",
    "        for label in labels:\n",
    "            if label.lower() in text.lower():\n",
    "                found = label\n",
    "                break\n",
    "        if not found:\n",
    "            found = \"FAIL\"\n",
    "        results.append(found)\n",
    "    return results\n",
    "\n",
    "# ---- Extract labels ----\n",
    "predicted_labels = clean_output(raw_outputs)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1-score': 0.0,\n",
       "  'support': 0.0},\n",
       " 'neutral': {'precision': 1.0,\n",
       "  'recall': 0.3333333333333333,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 9.0},\n",
       " 'positive': {'precision': 0.3333333333333333,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 1.0},\n",
       " 'accuracy': 0.4,\n",
       " 'macro avg': {'precision': 0.4444444444444444,\n",
       "  'recall': 0.4444444444444444,\n",
       "  'f1-score': 0.3333333333333333,\n",
       "  'support': 10.0},\n",
       " 'weighted avg': {'precision': 0.9333333333333333,\n",
       "  'recall': 0.4,\n",
       "  'f1-score': 0.5,\n",
       "  'support': 10.0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def compute_metrics(label_experts, label_pred):\n",
    "    # classification report gives us both aggregate and per-class metrics \n",
    "    metrics_report = classification_report(\n",
    "        label_experts, label_pred, digits=2, output_dict=True, zero_division='warn'\n",
    "    )\n",
    "    return metrics_report\n",
    "\n",
    "label_experts = dataset[\"label_text\"][:10]\n",
    "label_pred = predicted_labels\n",
    "\n",
    "metrics = compute_metrics(label_experts, label_pred)\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
