{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "## Important libs ##\n",
    "import os\n",
    "from pathlib import Path\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "os.chdir(Path.cwd().parent)\n",
    "\n",
    "from src.utils import load_env_file\n",
    "\n",
    "load_env_file()\n",
    "api_key = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "huggingface_hub.login(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic (annotated) Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label', 'label_text'],\n",
      "    num_rows: 2264\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", split='train', trust_remote_code=True).shuffle(seed=42)\n",
    "\n",
    "#dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)\n",
    "\n",
    "# create a new column with the numeric label verbalised as label_text (e.g. \"positive\" instead of \"0\")\n",
    "label_map = {\n",
    "    i: label_text \n",
    "    for i, label_text in enumerate(dataset.features[\"label\"].names)\n",
    "}\n",
    "\n",
    "def add_label_text(example):\n",
    "    example[\"label_text\"] = label_map[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_label_text)\n",
    "\n",
    "print(dataset)\n",
    "# Dataset({\n",
    "#    features: ['sentence', 'label', 'label_text'],\n",
    "#    num_rows: 2264\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'negative',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'positive',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'positive']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label_text'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"You are a highly qualified expert trained to annotate machine learning training data.\"\n",
    "prompt_financial_sentiment = \"\"\"\\\n",
    "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
    "positive, negative, or neutral.\n",
    "\n",
    "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
    "\n",
    "Do not provide any explanations and ONLY respond with one of the labels as one word: negative, positive, or neutral\n",
    "\n",
    "Examples:\n",
    "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
    "Label: positive\n",
    "Text: The company generated net sales of 11.3 million euro this year.\n",
    "Label: neutral\n",
    "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
    "Label: negative\n",
    "\n",
    "Your TEXT to analyse:\n",
    "TEXT: {text}\n",
    "Label: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_financial_sentiment = \"\"\"\\\n",
    "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
    "positive, negative, or neutral.\n",
    "\n",
    "ONLY respond with one of the labels as one word: negative, positive, or neutral. DONT BRING ANYTHING ELSE IN THE ANSWER, JUST THE LABEL\n",
    "\n",
    "If there is no comparison (comparing previous year with current year, or comparing two companies results), we can assume that the comment is neutral.\n",
    "\n",
    "Examples:\n",
    "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
    "Answer: positive\n",
    "Text: The company generated net sales of 11.3 million euro this year.\n",
    "Answer: neutral\n",
    "Text: There are 100 companies listed in the stock exchange.\n",
    "Answer: neutral\n",
    "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
    "Answer: negative\n",
    "\n",
    "Your TEXT to analyse:\n",
    "TEXT: {text}\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "# chat_financial_sentiment = [{\"role\": \"user\", \"content\": prompt_financial_sentiment}]\n",
    "\n",
    "# prompt_financial_sentiment = tokenizer.apply_chat_template(chat_financial_sentiment, tokenize=False)\n",
    "\n",
    "# The prompt now includes special tokens: '<s>[INST] You are a highly qualified expert ...  [/INST]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7682ed153544a1d91f70211514ea7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\" #\"microsoft/Phi-3-mini-4k-instruct\"\n",
    "torch.random.manual_seed(0) \n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    model_id,  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,  \n",
    ") \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "# ---- Cleaning function (maps to labels) ----\n",
    "def clean_output(outputs, labels=(\"positive\", \"negative\", \"neutral\")):\n",
    "    results = []\n",
    "    for out in outputs:\n",
    "        text = out[0][\"generated_text\"].strip()  # each out is a list of dicts\n",
    "        found = None\n",
    "        for label in labels:\n",
    "            if label.lower() in text.lower():\n",
    "                found = label\n",
    "                break\n",
    "        if not found:\n",
    "            found = \"FAIL\"\n",
    "        results.append(found)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Certainly! Bananas and dragonfruits can be combined in various delicious ways. Here are some creative ideas for incorporating both fruits into your meals or snacks:\n",
      "\n",
      "1. Smoothie: Blend together a ripe banana, a few slices of dragon fruit, a handful of spinach or kale, a splash of almond milk, and a tablespoon of honey or agave syrup for sweetness. Add a scoop of your favorite protein powder or a handful of ice for a refreshing and nutritious smoothie.\n",
      "\n",
      "2. Fruit Salad: Slice a ripe banana and a few pieces of dragon fruit, and combine them with other fruits like strawberries, blueberries, and kiwi. Toss the fruits with a drizzle of honey and a squeeze of lime juice for a colorful and flavorful fruit salad.\n",
      "\n",
      "3. Tropical Salsa: Dice a ripe banana and a few pieces of dragon fruit, and combine them with diced mango, pineapple, and red bell pepper. Add a squeeze of lime juice, a drizzle of honey, and a sprinkle of chopped cilantro for a sweet and tangy tropical salsa. Serve with tortilla chips or as a topping for grilled chicken or fish.\n",
      "\n",
      "4. Breakfast Bowl: Top a bowl of oatmeal or yogurt with sliced bananas and dragon fruit. Add a drizzle of honey, a sprinkle of cinnamon, and a handful of chopped nuts or seeds for a nutritious and satisfying breakfast bowl.\n",
      "\n",
      "5. Dessert Parfait: Layer slices of banana and dragon fruit with Greek yogurt, granola, and a drizzle of honey in a glass or jar. Repeat the layers until the glass is filled, and top with a sprinkle of chopped nuts or coconut flakes for a delicious and healthy dessert parfait.\n",
      "\n",
      "6. Fruit Skewers: Thread slices of banana and dragon fruit onto\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"}, \n",
    "] \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    \"temperature\": 0.9, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "\n",
    "output = pipe(messages, **generation_args) \n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Foundries division reports its sales increased by 9.7 % to EUR 63.1 mn from EUR 57.5 mn in the corresponding period in 2006 , and sales of the Machine Shop division increased by 16.4 % to EUR 41.2 mn from EUR 35.4 mn in the corresponding period in 2006 .\n",
      "Prediction: positive\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "one_shot_text = dataset[\"sentence\"][15]\n",
    "\n",
    "generation_params = dict(\n",
    "    top_p=0.95,\n",
    "    temperature=0.4,\n",
    "    max_new_tokens=128,\n",
    "    return_full_text=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "prompt_formatted = prompt_financial_sentiment.format(text=one_shot_text)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a highly qualified expert trained to annotate machine learning training data.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_formatted},\n",
    "]\n",
    "\n",
    "output = pipe(messages, **generation_params)\n",
    "\n",
    "\n",
    "#output = pipe(prompt_formatted, **generation_params)\n",
    "label = clean_output([output])\n",
    "print(f\"Input: {one_shot_text}\")\n",
    "print(f\"Prediction: {output[0]['generated_text'].strip()}\")\n",
    "print(\"-\" * 40)\n",
    "# print(f\"Text: {one_shot_text}\\nLabel: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 300\n",
    "\n",
    "### 100 samples took 3min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---- Build batch prompts (messages style) ----\n",
    "batch_messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a highly qualified expert trained to annotate machine learning training data.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_financial_sentiment.format(text=text)},\n",
    "    ]\n",
    "    for text in dataset[\"sentence\"][:N_samples]\n",
    "]\n",
    "\n",
    "# ---- Run batch inference ----\n",
    "raw_outputs = pipe(batch_messages, **generation_params)\n",
    "\n",
    "# ---- Extract labels ----\n",
    "predicted_labels = clean_output(raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 12, 16]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_pred = [i for i, k in enumerate(predicted_labels) if k == \"negative\"]\n",
    "negative_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_negative = [i for i, k in enumerate(label_experts) if k == \"negative\"]\n",
    "label_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 0.75,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.8571428571428571,\n",
       "  'support': 15.0},\n",
       " 'neutral': {'precision': 0.9545454545454546,\n",
       "  'recall': 0.7924528301886793,\n",
       "  'f1-score': 0.865979381443299,\n",
       "  'support': 53.0},\n",
       " 'positive': {'precision': 0.8055555555555556,\n",
       "  'recall': 0.90625,\n",
       "  'f1-score': 0.8529411764705882,\n",
       "  'support': 32.0},\n",
       " 'accuracy': 0.86,\n",
       " 'macro avg': {'precision': 0.8367003367003368,\n",
       "  'recall': 0.8995676100628932,\n",
       "  'f1-score': 0.8586878050189148,\n",
       "  'support': 100.0},\n",
       " 'weighted avg': {'precision': 0.8761868686868687,\n",
       "  'recall': 0.86,\n",
       "  'f1-score': 0.8604816772069652,\n",
       "  'support': 100.0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(label_experts, label_pred):\n",
    "    # classification report gives us both aggregate and per-class metrics \n",
    "    metrics_report = classification_report(\n",
    "        label_experts, label_pred, digits=2, output_dict=True, zero_division='warn'\n",
    "    )\n",
    "    return metrics_report\n",
    "\n",
    "label_experts = dataset[\"label_text\"][:N_samples]\n",
    "label_pred = predicted_labels\n",
    "\n",
    "metrics = compute_metrics(label_experts, label_pred)\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': {'precision': 0.6923076923076923,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.8181818181818182,\n",
       "  'support': 54.0},\n",
       " 'neutral': {'precision': 0.9705882352941176,\n",
       "  'recall': 0.7764705882352941,\n",
       "  'f1-score': 0.8627450980392157,\n",
       "  'support': 170.0},\n",
       " 'positive': {'precision': 0.7790697674418605,\n",
       "  'recall': 0.881578947368421,\n",
       "  'f1-score': 0.8271604938271605,\n",
       "  'support': 76.0},\n",
       " 'accuracy': 0.8433333333333334,\n",
       " 'macro avg': {'precision': 0.8139885650145567,\n",
       "  'recall': 0.8860165118679051,\n",
       "  'f1-score': 0.8360291366827316,\n",
       "  'support': 300.0},\n",
       " 'weighted avg': {'precision': 0.8719797257006558,\n",
       "  'recall': 0.8433333333333334,\n",
       "  'f1-score': 0.8457089412644969,\n",
       "  'support': 300.0}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(label_experts, label_pred):\n",
    "    # classification report gives us both aggregate and per-class metrics \n",
    "    metrics_report = classification_report(\n",
    "        label_experts, label_pred, digits=2, output_dict=True, zero_division='warn'\n",
    "    )\n",
    "    return metrics_report\n",
    "\n",
    "label_experts = dataset[\"label_text\"][:N_samples]\n",
    "label_pred = predicted_labels\n",
    "\n",
    "metrics = compute_metrics(label_experts, label_pred)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Accuracy of 86% achieved with the Phi-3-4k model\n",
    "- Latency is still a problem, it can take 30s to classify a text, for batch inference for thousands of articles, that can be a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Language Model\n",
    "\n",
    "We will use the annotated dataset from the previous step (pretending that we don't have the ground truth labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
